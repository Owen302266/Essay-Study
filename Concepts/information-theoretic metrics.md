Information-Theoretic Metrics

[[law of large numbers]]

$-\frac{1}{N} \sum_{n=1}^{N} \ln p\left(\mathrm{t}_{n} \mid \mathrm{x}_{n}, w, \beta\right) \rightarrow \mathrm{E}_{(\mathrm{x}, \mathrm{t}) \sim p_{\mathrm{xt}}}[-\ln p(\mathrm{t} \mid \mathrm{x}, w, \beta)]$

[[KL divergence]]

-> $\mathrm{E}_{(\mathrm{x}, \mathrm{t}) \sim p_{\mathrm{xt}}}[-\ln p(\mathrm{t} \mid \mathrm{x}, w, \beta)]=\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[H(p(t \mid \mathrm{x}) \| p(t \mid \mathrm{x}, w, \beta))]$

We don't distinct entropy and [[differential entropy]]