# 基础

## 线性回归

基于线性回归的框架进行后续的分析。

loss function损失函数  $\ell(t, \hat{t})$ 
* 常用损失q范数损失$\ell_{q}(t, \hat{t})=|t-\hat{t}|^{q}$
* $q=2$时，quadratic loss二次损失$\ell_{2}(t, \hat{t})=(t-\hat{t})^{2}$

算法优化->减小generalization risk or generalization loss泛化损失

### 频率论方法

$p(x,t)$求法：
1.Separate learning and (plug-in) inference独立学习和代入推理
2.Direct inference via Empirical Risk Minimization ([ERM])经验风险最小化直接推理
	empirical loss经验损失：$L_{\mathcal{D}}(\hat{t})=\frac{1}{N} \sum_{n=1}^{N} \ell\left(t_{n}, \hat{t}\left(x_{n}\right)\right)$

Discriminative vs. Generative Probabilistic Models判别模型与生成模型
1.判别式模型：直接学习后验分布
2.生成式模型：根据联合分布、先验分布计算分布从而进行区分

模型阶数与模型参数
1.[[Model order]] $M$ 模型阶数（hyperparameter超参数）
2.Model parameter $\theta$ 模型参数(假设)

#### Maximum Likelihood (ML) Learning最大似然学习
根据不同点的数据， 取对数得到Log-Likelihood (LL) function对数似然函数：
$\ln p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, w, \beta\right) = \sum_{n = 1}^{N} \ln p\left(t_{n} \mid x_{n}, w, \beta\right) \\ = -\frac{\beta}{2} \sum_{n = 1}^{N}\left(\mu\left(x_{n}, w\right)-t_{n}\right)^{2}+\frac{N}{2} \ln \frac{\beta}{2 \pi}$

待优化参数：$\theta=(w, \beta)$
ML问题等价于最小化Negative LL (NLL)负面影响函数，或者[[cross-entropy]]交叉熵、log-loss对数损失。

只关注$w$时，优化器：$\min _{w} L_{\mathcal{D}}(w)=\frac{1}{N} \sum_{n=1}^{N}\left(\mu\left(x_{n}, w\right)-t_{n}\right)^{2}$，优化目标是$L_{\mathcal{D}}(w)$，即训练损失。
当$\hat{t}(x)=\mu(x, w)$时，与[ERM]算法一致。

[ERM]可以用封闭形式求解：
1.将损失函数改写：$L_{\mathcal{D}}(w)=N^{-1}\left\|t_{\mathcal{D}}-X_{\mathcal{D}} w\right\|^{2}$
2.通过Least Squares (LS)最小二乘法求解：$w_{M L}=\left(X_{\mathcal{D}}^{T} X_{\mathcal{D}}\right)^{-1} X_{\mathcal{D}}^{T} t_{\mathcal{D}}$
3.对$\beta$进行估计，得到ML估计：$\frac{1}{\beta_{M L}}=L_{\mathcal{D}}\left(w_{M L}\right)$

Overfitting and Underfitting过拟合与欠拟合：
欠拟合：不能捕获数据中存在的变化，训练损失$L_{\mathcal{D}}\left(w_{M L}\right)$较大
过拟合：在训练集之外产生了不准确的预测，训练损失$L_{\mathcal{D}}\left(w_{M L}\right)$较小，但泛化损失$L_{p}\left(w_{M L}\right)=\mathrm{E}_{(\mathrm{x}, \mathrm{t}) \sim p_{\mathrm{xt}}}\left[\ell\left(\mathrm{t}, \mu\left(\mathrm{x}, w_{M L}\right)\right)\right]$较大。过拟合情况下是在记忆训练集，并非推广。

泛化损失$L_{p}\left(w_{M L}\right)$与训练损失$L_{\mathcal{D}}\left(w_{M L}\right)$的变化关系：
1.训练损失小于泛化损失：泛化损失是对全体数据而训练损失只针对训练数据。
2.增加模型阶数$M$会更好的拟合，但可能会过拟合。
3.在没有欠拟合的情况下泛化损失$L_{p}\left(w_{M L}\right)$会先减小后增大

样本点数$N$增大的影响：
表述1：只要$N$足够大，最小化的训练损失$L_{\mathcal{D}}(w)$的权重向量$w_{M L}$近似等于最小化的泛化损失$L_{p}(w)$。
表述2：对于足够大的样本点数，ML参数向量$w_{M L}$趋向于最优值$w^*$：$w^{*}=\arg \min _{w} L_{p}(w)$。
解释：
1.过拟合--通过训练和泛化损失之间的差距来衡量--随着N的增加而消失。
2.对于给定的$M$，训练损失$L_{\mathcal{D}}(w)$从下方趋向于最小泛化损失$L_{p}\left(w^{*}\right)$，而泛化损失$L_{p}(w)$从下方趋近。
3.训练集较小时，选择较小的模型阶数$M$改进泛化损失，数据集较大时，需要的模型阶数也越大。

Bias and generalization gap误差与泛化差距
对泛化损失进行分解：$L_{p}\left(w_{M L}\right)=L_{p}\left(\hat{t}^{*}\right)+\left(L_{p}\left(w^{*}\right)-L_{p}\left(\hat{t}^{*}\right)\right)+\left(L_{p}\left(w_{M L}\right)-L_{p}\left(w^{*}\right)\right)$
$L_{p}\left(\hat{t}^{*}\right)$：假设类没有任何约束时可达到的最小推广损失。
$\left(L_{p}\left(w^{*}\right)-L_{p}\left(\hat{t}^{*}\right)\right)$：由给定假设类的选择引起的偏差或近似误差，与选择的阶数$M$有关。
$\left(L_{p}\left(w_{M L}\right)-L_{p}\left(w^{*}\right)\right)$：由于N不够大而导致的估计误差或推广间隙。

Validation and testing验证与测试
method1:将可用数据分成两组：验证组与训练，验证集采用经验平均值评估泛化损失的近似值：
$L_{p}(w) \simeq \frac{1}{N_{v}} \sum_{n=1}^{N_{v}} \ell\left(t_{n}, \mu\left(x_{n}, w\right)\right)$
method2:k-fold cross-validation k重交叉验证
随机地将可用数据点划分为$k$个相等大小的子集。然后通过平均$k$个不同的估计值来估计泛化损失。每个估计值都是通过保留$k$个子集中的一个用于验证，其余$k − 1$个子集用于训练而获得的。
当$k=N$时，被称为leave-one-out cross-validation留一法交叉验证

ML的问题：在ML学习中关于模型阶数M的决定涉及偏差和估计误差之间的张力，偏差的减小需要更大的M，估计误差的减小需要更小的M。ML提供单个整数参数M作为权衡偏差和估计误差的度量。

#### Maximum A Posteriori (MAP) Criterion最大后验准则

过拟合的表现是权重向量的范数$\|w\|$的大值，因此在设计算法的时候可以用作先验信息。
在权重向量上施加先验分布，如高斯分布和拉普拉斯分布。

先验分布为方差为$\alpha^{-1}$的零均值高斯分布：$w \sim \mathcal{N}\left(0, \alpha^{-1} I\right)$，$\alpha$是超参数，增大会是权重变小。
改写ML的优化函数为：$p\left(t_{\mathcal{D}}, w \mid x_{\mathcal{D}}, \beta\right)=p(w) \prod_{n=1}^{N} p\left(t_{n} \mid x_{n}, w, \beta\right)$
最大后验学习准则为：$\min _{w, \beta}-\sum_{n=1}^{N} \ln p\left(t_{n} \mid x_{n}, w, \beta\right)-\ln p(w)$
改写为权向量问题（岭回归问题）：$\min _{w} L_{\mathcal{D}}(w)+\frac{\lambda}{N}\|w\|^{2}$，其中$\lambda=\alpha / \beta$
范数正则化项为：$R(w)=\|w\|^{2}$
采用标准LS最小二乘求解：$w_{M A P}=\left(\lambda I+X_{\mathcal{D}}^{T} X_{\mathcal{D}}\right)^{-1} X_{\mathcal{D}}^{T} t_{\mathcal{D}}$

超参数的影响：
增加λ，从而增加正则化项的相关性，与降低模型阶数M具有相似的影响。较大的λ会降低模型的有效容量。换句话说，增加λ会减少过拟合，但可能会导致更大的偏差。

ML与MAP的关系：
当数据点的数目N变大时，MAP估计趋向于ML估计，假定先验信息项的贡献按1/N减小。当N足够大时，任何先前的信任因此被从数据中获得的信息所取代。$w_{M A P} \rightarrow w_{M L}$

先验分布为拉普拉斯分布：
范数正则化函数：$R(w)=\|w\|_{1}=\sum_{j=0}^{M}|w|_{1}$
优化问题为：$\min _{w} L_{\mathcal{D}}(w)+\frac{\lambda}{N}\|w\|_{1}$
也就是[[LASSO]] (Least Absolute Shrinkage and Selection Operator)

#### [[regularization]]正则化

正则化通常指的是旨在减少训练期间的过拟合的技术。如岭回归、[[LASSO]]、在ERM公式中独立于概率框架引入等方法。

其他优化方案：
1.使用early stopping等方法
2.通过生成人工实例来扩充训练集，从而有效地增加训练实例的数量N。（bagging方法）
	1.首先创建K个bootstrap数据集，通过均匀地选择N个数据点并从D中进行替换而获得的（使得相同的数据点通常在Bootstrap数据集中出现多次）。
	2.训练模型K次，每次都在不同的[[bootstrap]]集上。
	3.使用相等的权重对从模型获得的结果进行平均。
	若不同模型间相互独立，产生随K减小的估计误差，在实践中获得的增益较小。

### 贝叶斯方法

频率论数据的两种分布：真实分布，由数据和模型的经验分布近似。

贝叶斯观点的不同：
1.假设所有数据点都按照一个已知的分布联合分布，超参数不同
2.模型$\theta$与数据联合分布
因此贝叶斯方法考虑$\theta$的所有可能值提供的解释，而不是承诺用单个值来解释数据，每个值根据通常不同的、依赖于数据的权重进行加权。

训练集中的标签、权重向量和新标签的联合分布为：
$p\left(t_{\mathcal{D}}, w, t \mid x_{\mathcal{D}}, x\right)=\underbrace{p(w)}_{\text {a priori distribution }} \underbrace{p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, w\right)}_{\text {likelihood }} \underbrace{p(t \mid x, w)}_{\text {distribution of new data }}$
去掉对域点$x_{\mathcal{D}}$的依赖：$p\left(t_{\mathcal{D}}, w, t\right)=\underbrace{p(w)}_{\text {a priori distribution }} \underbrace{p\left(t_{\mathcal{D}} \mid w\right)}_{\text {likelihood }} \underbrace{p(t \mid w)}_{\text {distribution of new data }}$
去掉对域点变量x的依赖：$p\left(t \mid t_{\mathcal{D}}\right)=\frac{p\left(t_{\mathcal{D}}, t\right)}{p\left(t_{\mathcal{D}}\right)}=\int \frac{p(w) p\left(t_{\mathcal{D}} \mid w\right)}{p\left(t_{\mathcal{D}}\right)} p(t \mid w) d w \\=\int p\left(w \mid t_{\mathcal{D}}\right) p(t \mid w)dw$
再带回自变量的影响：$p(w \mid \mathcal{D})=\frac{p(w) p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, w\right)}{p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}\right)}$
通常计算后验概率是很难的，因此需要用到后面的近似推理等方法。

ML&MAP与Bayesian的区别

ML&MAP：返回值$p\left(t \mid x, \theta_{M L}\right)=\mathcal{N}\left(\mu\left(x, w_{M L}\right), \beta^{-1}\right)$，对与所有x值都有相同的方差$\beta^{-1}$
Bayesian：
1.由于x的观测值的不均匀分布，标签预测的准确性取决于x的值：x的值越接近训练集中的现有点，通常表现出越小的方差。
2.允许在没有验证的情况下执行模型选择，但实际中仍然需要自己选择参数。

Marginal likelihood边际似然
边际似然计算方法：$p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}\right)=\int p(w) \prod_{n=1}^{N} p\left(t_{n} \mid x_{n}, w\right) d w$
较大的M意味着权重的先验分布更加“分散”，这可能导致式中的标签分布更加分散。因此，增加M可能产生较小的边际似然性。
边际似然在M的给定值处呈现峰值，而当远离最优值时降低。因此，我们可以将边际似然最大化时的M值作为所选模型阶数。

Empirical Bayes经验贝叶斯：
假设参数为先验分布，但随后根据数据估计先验的参数-例如高斯先验的均值和方差。

### Minimum Description Length (MDL)最小描述长度

由MDL选择的模型定义了用最小数量的比特描述数据集D的压缩方案
选择能对数据产生最简单解释的模型，该标准自然导致惩罚过拟合的解决方案。









## 概率学习模型


# 有监督学习

## 分类


## 统计学习理论

# 无监督学习


## 无监督学习

# 高级建模和推理

## 概率图模型

## 近似推理与学习



