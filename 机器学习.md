# 基础

## 线性回归

基于线性回归的框架进行后续的分析。

loss function损失函数  $\ell(t, \hat{t})$ 
* 常用损失q范数损失$\ell_{q}(t, \hat{t})=|t-\hat{t}|^{q}$
* $q=2$时，quadratic loss二次损失$\ell_{2}(t, \hat{t})=(t-\hat{t})^{2}$

算法优化->减小generalization risk or generalization loss泛化损失

### 频率论方法

$p(x,t)$求法：
1.Separate learning and (plug-in) inference独立学习和代入推理
2.Direct inference via Empirical Risk Minimization ([ERM])经验风险最小化直接推理
	empirical loss经验损失：$L_{\mathcal{D}}(\hat{t})=\frac{1}{N} \sum_{n=1}^{N} \ell\left(t_{n}, \hat{t}\left(x_{n}\right)\right)$

Discriminative vs. Generative Probabilistic Models判别模型与生成模型
1.判别式模型：直接学习后验分布
2.生成式模型：根据联合分布、先验分布计算分布从而进行区分

模型阶数与模型参数
1.[[Model order]] $M$ 模型阶数（hyperparameter超参数）
2.Model parameter $\theta$ 模型参数(假设)

#### Maximum Likelihood (ML) Learning最大似然学习
根据不同点的数据， 取对数得到Log-Likelihood (LL) function对数似然函数：
$\ln p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, w, \beta\right) = \sum_{n = 1}^{N} \ln p\left(t_{n} \mid x_{n}, w, \beta\right) \\ = -\frac{\beta}{2} \sum_{n = 1}^{N}\left(\mu\left(x_{n}, w\right)-t_{n}\right)^{2}+\frac{N}{2} \ln \frac{\beta}{2 \pi}$

待优化参数：$\theta=(w, \beta)$
ML问题等价于最小化Negative LL (NLL)负面影响函数，或者[[cross-entropy]]交叉熵、log-loss对数损失。

最小化NLL函数：$\min _{w, \beta}-\frac{1}{N} \sum_{n=1}^{N} \ln p\left(t_{n} \mid x_{n}, w, \beta\right)$

只关注$w$时，优化器：$\min _{w} L_{\mathcal{D}}(w)=\frac{1}{N} \sum_{n=1}^{N}\left(\mu\left(x_{n}, w\right)-t_{n}\right)^{2}$，优化目标是$L_{\mathcal{D}}(w)$，即训练损失。
当$\hat{t}(x)=\mu(x, w)$时，与[ERM]算法一致。

[ERM]可以用封闭形式求解：
1.将损失函数改写：$L_{\mathcal{D}}(w)=N^{-1}\left\|t_{\mathcal{D}}-X_{\mathcal{D}} w\right\|^{2}$
2.通过Least Squares (LS)最小二乘法求解：$w_{M L}=\left(X_{\mathcal{D}}^{T} X_{\mathcal{D}}\right)^{-1} X_{\mathcal{D}}^{T} t_{\mathcal{D}}$
3.对$\beta$进行估计，得到ML估计：$\frac{1}{\beta_{M L}}=L_{\mathcal{D}}\left(w_{M L}\right)$

Overfitting and Underfitting过拟合与欠拟合：
欠拟合：不能捕获数据中存在的变化，训练损失$L_{\mathcal{D}}\left(w_{M L}\right)$较大
过拟合：在训练集之外产生了不准确的预测，训练损失$L_{\mathcal{D}}\left(w_{M L}\right)$较小，但泛化损失$L_{p}\left(w_{M L}\right)=\mathrm{E}_{(\mathrm{x}, \mathrm{t}) \sim p_{\mathrm{xt}}}\left[\ell\left(\mathrm{t}, \mu\left(\mathrm{x}, w_{M L}\right)\right)\right]$较大。过拟合情况下是在记忆训练集，并非推广。

泛化损失$L_{p}\left(w_{M L}\right)$与训练损失$L_{\mathcal{D}}\left(w_{M L}\right)$的变化关系：
1.训练损失小于泛化损失：泛化损失是对全体数据而训练损失只针对训练数据。
2.增加模型阶数$M$会更好的拟合，但可能会过拟合。
3.在没有欠拟合的情况下泛化损失$L_{p}\left(w_{M L}\right)$会先减小后增大

样本点数$N$增大的影响：
表述1：只要$N$足够大，最小化的训练损失$L_{\mathcal{D}}(w)$的权重向量$w_{M L}$近似等于最小化的泛化损失$L_{p}(w)$。
表述2：对于足够大的样本点数，ML参数向量$w_{M L}$趋向于最优值$w^*$：$w^{*}=\arg \min _{w} L_{p}(w)$。
解释：
1.过拟合--通过训练和泛化损失之间的差距来衡量--随着N的增加而消失。
2.对于给定的$M$，训练损失$L_{\mathcal{D}}(w)$从下方趋向于最小泛化损失$L_{p}\left(w^{*}\right)$，而泛化损失$L_{p}(w)$从下方趋近。
3.训练集较小时，选择较小的模型阶数$M$改进泛化损失，数据集较大时，需要的模型阶数也越大。

Bias and generalization gap误差与泛化差距
对泛化损失进行分解：$L_{p}\left(w_{M L}\right)=L_{p}\left(\hat{t}^{*}\right)+\left(L_{p}\left(w^{*}\right)-L_{p}\left(\hat{t}^{*}\right)\right)+\left(L_{p}\left(w_{M L}\right)-L_{p}\left(w^{*}\right)\right)$
$L_{p}\left(\hat{t}^{*}\right)$：假设类没有任何约束时可达到的最小推广损失。
$\left(L_{p}\left(w^{*}\right)-L_{p}\left(\hat{t}^{*}\right)\right)$：由给定假设类的选择引起的偏差或近似误差，与选择的阶数$M$有关。
$\left(L_{p}\left(w_{M L}\right)-L_{p}\left(w^{*}\right)\right)$：由于N不够大而导致的估计误差或推广间隙。

Validation and testing验证与测试
method1:将可用数据分成两组：验证组与训练，验证集采用经验平均值评估泛化损失的近似值：
$L_{p}(w) \simeq \frac{1}{N_{v}} \sum_{n=1}^{N_{v}} \ell\left(t_{n}, \mu\left(x_{n}, w\right)\right)$
method2:k-fold cross-validation k重交叉验证
随机地将可用数据点划分为$k$个相等大小的子集。然后通过平均$k$个不同的估计值来估计泛化损失。每个估计值都是通过保留$k$个子集中的一个用于验证，其余$k − 1$个子集用于训练而获得的。
当$k=N$时，被称为leave-one-out cross-validation留一法交叉验证

ML的问题：在ML学习中关于模型阶数M的决定涉及偏差和估计误差之间的张力，偏差的减小需要更大的M，估计误差的减小需要更小的M。ML提供单个整数参数M作为权衡偏差和估计误差的度量。

#### Maximum A Posteriori (MAP) Criterion最大后验准则

过拟合的表现是权重向量的范数$\|w\|$的大值，因此在设计算法的时候可以用作先验信息。
在权重向量上施加先验分布，如高斯分布和拉普拉斯分布。

先验分布为方差为$\alpha^{-1}$的零均值高斯分布：$w \sim \mathcal{N}\left(0, \alpha^{-1} I\right)$，$\alpha$是超参数，增大会是权重变小。
改写ML的优化函数为：$p\left(t_{\mathcal{D}}, w \mid x_{\mathcal{D}}, \beta\right)=p(w) \prod_{n=1}^{N} p\left(t_{n} \mid x_{n}, w, \beta\right)$
最大后验学习准则为：$\min _{w, \beta}-\sum_{n=1}^{N} \ln p\left(t_{n} \mid x_{n}, w, \beta\right)-\ln p(w)$
改写为权向量问题（岭回归问题）：$\min _{w} L_{\mathcal{D}}(w)+\frac{\lambda}{N}\|w\|^{2}$，其中$\lambda=\alpha / \beta$
范数正则化项为：$R(w)=\|w\|^{2}$
采用标准LS最小二乘求解：$w_{M A P}=\left(\lambda I+X_{\mathcal{D}}^{T} X_{\mathcal{D}}\right)^{-1} X_{\mathcal{D}}^{T} t_{\mathcal{D}}$

超参数的影响：
增加λ，从而增加正则化项的相关性，与降低模型阶数M具有相似的影响。较大的λ会降低模型的有效容量。换句话说，增加λ会减少过拟合，但可能会导致更大的偏差。

ML与MAP的关系：
当数据点的数目N变大时，MAP估计趋向于ML估计，假定先验信息项的贡献按1/N减小。当N足够大时，任何先前的信任因此被从数据中获得的信息所取代。$w_{M A P} \rightarrow w_{M L}$

先验分布为拉普拉斯分布：
范数正则化函数：$R(w)=\|w\|_{1}=\sum_{j=0}^{M}|w|_{1}$
优化问题为：$\min _{w} L_{\mathcal{D}}(w)+\frac{\lambda}{N}\|w\|_{1}$
也就是[[LASSO]] (Least Absolute Shrinkage and Selection Operator)

#### [[regularization]]正则化

正则化通常指的是旨在减少训练期间的过拟合的技术。如岭回归、[[LASSO]]、在ERM公式中独立于概率框架引入等方法。

其他优化方案：
1.使用early stopping等方法
2.通过生成人工实例来扩充训练集，从而有效地增加训练实例的数量N。（bagging方法）
	1.首先创建K个bootstrap数据集，通过均匀地选择N个数据点并从D中进行替换而获得的（使得相同的数据点通常在Bootstrap数据集中出现多次）。
	2.训练模型K次，每次都在不同的[[bootstrap]]集上。
	3.使用相等的权重对从模型获得的结果进行平均。
	若不同模型间相互独立，产生随K减小的估计误差，在实践中获得的增益较小。

### 贝叶斯方法

频率论数据的两种分布：真实分布，由数据和模型的经验分布近似。

贝叶斯观点的不同：
1.假设所有数据点都按照一个已知的分布联合分布，超参数不同
2.模型$\theta$与数据联合分布
因此贝叶斯方法考虑$\theta$的所有可能值提供的解释，而不是承诺用单个值来解释数据，每个值根据通常不同的、依赖于数据的权重进行加权。

训练集中的标签、权重向量和新标签的联合分布为：
$p\left(t_{\mathcal{D}}, w, t \mid x_{\mathcal{D}}, x\right)=\underbrace{p(w)}_{\text {a priori distribution }} \underbrace{p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, w\right)}_{\text {likelihood }} \underbrace{p(t \mid x, w)}_{\text {distribution of new data }}$
去掉对域点$x_{\mathcal{D}}$的依赖：$p\left(t_{\mathcal{D}}, w, t\right)=\underbrace{p(w)}_{\text {a priori distribution }} \underbrace{p\left(t_{\mathcal{D}} \mid w\right)}_{\text {likelihood }} \underbrace{p(t \mid w)}_{\text {distribution of new data }}$
去掉对域点变量x的依赖：$p\left(t \mid t_{\mathcal{D}}\right)=\frac{p\left(t_{\mathcal{D}}, t\right)}{p\left(t_{\mathcal{D}}\right)}=\int \frac{p(w) p\left(t_{\mathcal{D}} \mid w\right)}{p\left(t_{\mathcal{D}}\right)} p(t \mid w) d w \\=\int p\left(w \mid t_{\mathcal{D}}\right) p(t \mid w)dw$
再带回自变量的影响：$p(w \mid \mathcal{D})=\frac{p(w) p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, w\right)}{p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}\right)}$
通常计算后验概率是很难的，因此需要用到后面的近似推理等方法。

ML&MAP与Bayesian的区别

ML&MAP：返回值$p\left(t \mid x, \theta_{M L}\right)=\mathcal{N}\left(\mu\left(x, w_{M L}\right), \beta^{-1}\right)$，对与所有x值都有相同的方差$\beta^{-1}$
Bayesian：
1.由于x的观测值的不均匀分布，标签预测的准确性取决于x的值：x的值越接近训练集中的现有点，通常表现出越小的方差。
2.允许在没有验证的情况下执行模型选择，但实际中仍然需要自己选择参数。

Marginal likelihood边际似然
边际似然计算方法：$p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}\right)=\int p(w) \prod_{n=1}^{N} p\left(t_{n} \mid x_{n}, w\right) d w$
较大的M意味着权重的先验分布更加“分散”，这可能导致式中的标签分布更加分散。因此，增加M可能产生较小的边际似然性。
边际似然在M的给定值处呈现峰值，而当远离最优值时降低。因此，我们可以将边际似然最大化时的M值作为所选模型阶数。

Empirical Bayes经验贝叶斯：
假设参数为先验分布，但随后根据数据估计先验的参数-例如高斯先验的均值和方差。

### Minimum Description Length (MDL)最小描述长度

由MDL选择的模型定义了用最小数量的比特描述数据集D的压缩方案
选择能对数据产生最简单解释的模型，该标准自然导致惩罚过拟合的解决方案。

选择特定的M值后，对于给定的M值，有多少个概率分布就有多少个相应的参数$\theta$值可供选择。

two-part codes两部分代码，计算由模型阶数M的选择产生的数据D的描述长度。
1.得到ML解：$\left(w_{M L}, \beta_{M L}\right)$
2.通过使用由概率定义的压缩方案来描述数据集：$p\left(t \mid x, w_{M L}, \beta_{M L}\right)=\mathcal{N}\left(t \mid \mu\left(x, w_{M L}\right), \beta_{M L}^{-1}\right)$
近似描述为：$-\sum_{n=1}^{N} \log p\left(t_{n} \mid x_{n}, w_{M L}, \beta_{M L}\right)$
可以通过与参数的数量（这里为$M+2$）成比例的位数$C(M)$来量化描述参数。将这些位与ML模型生成的描述连接起来，即可得到总描述长度:$-\sum_{n=1}^{N} \log p\left(t_{n} \mid x_{n}, w_{M L}, \beta_{M L}\right)+C(M)$

用于MDL准则的$M$的最佳值是开销$C(M)$的最小化与NLL的最小化之间的折衷的结果，开销$C(M)$的最小化要求较小的$M$值，NLL的最小化随$M$减小。

### Information-Theoretic Metrics信息论度量

#### Information Measures信息测量
在专著中，我们采取了实用的方法，通过用积分代替和，将定义扩展到连续变量，这种方法在处理互信息和分歧时没有带来任何实际的复杂性。
熵的连续形式，称为微分熵，应该小心对待，因为它不满足熵的一些关键性质，如非负性。


##### Entropy熵

香农提出：从定义在有限字母表$\chi$上的离散随机变量$x\sim p(x)$的观测中获得的信息量，应该用测量前其值的不确定性来衡量。
关键思想是，如果随机变量x的值更难先验预测，即仅基于p（x）的知识，则对该随机变量x的观测更能提供信息。
需要提前指定：
(i)允许对x的值进行的估计的类型。
(ii)用于测量估计的准确度的损失函数。

Point Estimates点估计：
广义熵的定义：$H_{\ell}\left(p_{\mathrm{x}}\right)=\min _{\hat{x}} \mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[\ell(\mathrm{x}, \hat{x})]$，其概念与给定损失函数的最小贝叶斯风险一致。
对于二次损失函数，广义熵是分布$H_{\ell_{2}}\left(p_{\mathrm{x}}\right)=\operatorname{var}\left(p_{\mathrm{x}}\right)$的方差。
对于0-1损失函数，广义熵是检测x的最小错误概率：$H_{\ell_{0}}\left(p_{\mathrm{x}}\right)=\min _{\hat{x}} \sum_{x \neq \hat{x}} p(x)=1-\max _{\hat{x}} p(\hat{x})$

Distributional Estimate分布估计：
定义一个负增益损失函数：$\ell\left(x, \hat{p}_{\mathrm{x}}\right)=f(\hat{p}(x))$
将优化目标修改为x的分布：$H_{\ell}\left(p_{\mathrm{x}}\right)=\min _{\hat{p}_{\mathrm{x}} \in \Delta(\mathcal{X})} \mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}\left[\ell\left(\mathrm{x}, \hat{p}_{\mathrm{x}}\right)\right]$
对数损失在无损压缩方面具有很强的动机，上式中的期望值测量了给定方案的无损压缩所需的相应平均比特数。

香农熵：$\min _{\hat{p}_{\mathrm{x}} \in \Delta(\mathcal{X})} \mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[-\ln \hat{p}(\mathrm{x})] \\=\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[-\ln p(\mathrm{x})]=H\left(p_{\mathrm{x}}\right)$
在对所有可能的$\hat{p}(\mathrm{x})$优化时，熵$H(p_x)$是最小的平均对数损失。
只有选择对数损失函数时，$\hat{p}(\mathrm{x})=p(x)$才是最优解

广义熵$H_{\ell}\left(p_{\mathrm{x}}\right)$是凹函数，根据不等式：$H_{\ell}\left(\lambda p_{\mathrm{x}}+(1-\lambda) q_{\mathrm{x}}\right) \geq\lambda H_{\ell}\left(p_{\mathrm{x}}\right)+(1-\lambda) H_{\ell}\left(q_{\mathrm{x}}\right)$，按照两种分布的混合分布的变量的分布更加随机，相较两种原始分布更难估计

##### Conditional Entropy and Mutual Information条件熵和互信息

条件熵：
点估计：
给定损失函数，观测值y = $y$的条件熵被定义为最小平均损失：$H_{\ell}\left(p_{\mathrm{x} \mid \mathrm{y}=y}\right)=\min _{\hat{x}(y)} \mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x} \mid \mathrm{y}=y}}[\ell(\mathrm{x}, \hat{x}(y)) \mid \mathrm{y}=y]$
对观测值y的分布求平均值可得到广义条件熵（取决于联合分布$p_{xy}$）：$H_{\ell}(\mathrm{x} \mid \mathrm{y})=\mathrm{E}_{\mathrm{y} \sim p_{\mathrm{y}}}\left[H_{\ell}\left(p_{\mathrm{x} \mid \mathrm{y}}\right)\right]$
对于平方误差，广义条件熵可以很容易地看作是平均条件方差:$H_{\ell_{2}}(\mathrm{x} \mid \mathrm{y})=\mathrm{E}_{\mathrm{y} \sim p_{\mathrm{y}}}\left[\operatorname{var}\left(p_{\mathrm{x} \mid \mathrm{y}}\right)\right]$
对于0-1损失，广义条件熵$H_{\ell_{0}}(\mathrm{x} \mid \mathrm{y})$等于给定y检测x的最小误差概率且MAP估计$\hat{x}(y)=\operatorname{argmax}_{\hat{x} \in \mathrm{x}} p(\hat{x} \mid y)$时是最佳的。
分布估计：
广义条件熵的定义直接得出：$H_{\ell}\left(p_{\mathrm{x} \mid \mathrm{y}=y}\right)$
对数损失函数条件下：$H(\mathrm{x} \mid \mathrm{y})=\mathrm{E}_{\mathrm{x}, \mathrm{y} \sim p_{\mathrm{x}, \mathrm{y}}}[-\ln p(\mathrm{x} \mid \mathrm{y})]$

x与y独立的条件下，有不等式：$H_{\ell}(\mathrm{x} \mid \mathrm{y}) \leq H_{\ell}(\mathrm{x})$：观察y只能减少熵

Mutual Information互信息:
由上不等式，给定损失函数的广义互信息定义为：$I_{\ell}(\mathrm{x} ; \mathrm{y})=H_{\ell}(\mathrm{x})-H_{\ell}(\mathrm{x} \mid \mathrm{y})$
互信息测量与仅具有关于px的先验信息相比，通过观察y获得的平均损耗的减少。通过对数损失，广义互信息简化为香农互信息。

##### Divergence Measures散度度量
考虑两个分布之间的差异性，设计决策规则$T(x)$，最小化损失函数，等价的最大化价值函数：$\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[T(\mathrm{x})]-\mathrm{E}_{\mathrm{x} \sim q_{\mathrm{x}}}[g(T(\mathrm{x}))]$
函数g可用于定义有利于一种分布或另一种分布的误差的相对重要性,从而产生以下定义：$D_{f}\left(p_{\mathrm{x}} \| q_{\mathrm{x}}\right)=\max _{T(x)} \mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[T(\mathrm{x})]-\mathrm{E}_{\mathrm{x} \sim q_{\mathrm{x}}}[g(T(\mathrm{x}))]$
在$g$可微性假设的前提下，求导得到最优性条件$g^{\prime}(T(x))=p(x) / q(x)$，从而：$D_{f}\left(p_{\mathrm{x}} \| q_{\mathrm{x}}\right)=\mathrm{E}_{\mathrm{x} \sim q_{\mathrm{x}}}\left[f\left(\frac{p_{\mathrm{x}}(\mathrm{x})}{q_{\mathrm{x}}(\mathrm{x})}\right)\right]$，其中$f(x)=g^{*}(x)$，$g^{*}(x)=\sup _{t}(x t-g(t))$

选择g(x)即为不同的散度公式。
1.$g(t)=\exp (t-1)$，$T(x)=1+\ln (p(x) / q(x))$，为KL散度公式
2.$g(t) =− ln(2−exp(t))$，$T(x)=\ln \left(2 p_{\mathrm{x}}(x) /( p_{\mathrm{x}}(x)+q_{\mathrm{x}}(x)\right))$，为JS散度
与KL散度关系：$\operatorname{JS}\left(p_{\mathrm{x}} \| q_{\mathrm{x}}\right)=\mathrm{KL}\left(p_{\mathrm{x}} \| m_{\mathrm{x}}\right)+\mathrm{KL}\left(q_{\mathrm{x}} \| m_{\mathrm{x}}\right)$，其中$m_{\mathrm{x}}(x)=\left(p_{\mathrm{x}}(x)+q_{\mathrm{x}}(x)\right) / 2$
3.$f(x)=(\alpha(x-1)-\left.\left(x^{\alpha}-1\right)\right) /(\alpha(1-\alpha))$，为$\alpha$散度

#### KL散度

从ML问题的最小化NLL函数入手：
通过强大数定律，得到概率为1的[极限]：$-\frac{1}{N} \sum_{n=1}^{N} \ln p\left(\mathrm{t}_{n} \mid \mathrm{x}_{n}, w, \beta\right) \rightarrow \mathrm{E}_{(\mathrm{x}, \mathrm{t}) \sim p_{\mathrm{xt}}}[-\ln p(\mathrm{t} \mid \mathrm{x}, w, \beta)]$，可以用于解释KL散度。

KL散度是两个分布之间的对数似然比的期望值，该期望值是相对于分子处的分布而取的，该分布随着两个分布变得更加不同而增加。
$p$和$q$间的KL散度可以定义为：$\mathrm{KL}(p \| q)=\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}\left[\ln \frac{p(\mathrm{x})}{q(\mathrm{x})}\right]$
正态分布条件$p(x)=\mathcal{N}\left(x \mid \mu_{1}, \sigma_{1}^{2}\right)$,$q(x)=\mathcal{N}\left(x \mid \mu_{2}, \sigma_{2}^{2}\right)$下,KL散度公式可改写为：$\mathrm{KL}(p \| q)=\frac{1}{2}\left(\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}+\frac{\left(\mu_{2}-\mu_{1}\right)^{2}}{\sigma_{2}^{2}}-1+\ln \frac{\sigma_{2}^{2}}{\sigma_{1}^{2}}\right)$
性质：
1.Gibbs inequality：$K L(p \| q) \geq 0$当且仅当两个分布相同时等式成立。
2.非对称性：$K L(p \| q) \ne K L(q \| p)$，可以用来定义不同类型的近似推理和学习技巧。

与信息论的关系：KL散度公式可以化为：
$\mathrm{KL}(p \| q)=\underbrace{\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[-\ln q(\mathrm{x})]}_{H(p \| q)}-\underbrace{\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[-\ln p(\mathrm{x})]}_{H(p)}$
$H(p \| q)$：cross-entropy交叉熵，可以是负值
$H(p)$：分布$p(x)$的熵，是随机性的度量，对于离散型随机变量是非负的，但对于连续型随机变量可能是负值，因此对连续型随机变量进行评估时称为微分熵。

上述[极限]右侧的对数损失期望值可以表示为：$\mathrm{E}_{(\mathrm{x}, \mathrm{t}) \sim p_{\mathrm{xt}}}[-\ln p(\mathrm{t} \mid \mathrm{x}, w, \beta)]=\mathrm{E}_{\mathrm{x} \sim p_{\mathrm{x}}}[H(p(t \mid \mathrm{x}) \| p(t \mid \mathrm{x}, w, \beta))]$

根据上式，ML问题可以解释为试图使基于模型的判别分布$p(t|x,w,β)$尽可能接近实际后验$p(t| x)$的值。这是通过在$p(x)$上求平均值时最小化KL散度或等价地最小化交叉熵来实现的。

### Interpretation and Causality解释和因果关系

对学习算法返回的结果的解释：在评估x和t之间的因果关系时，我们首先应该了解哪些其他变量可以解释观察结果，然后对任何虚假的相关性进行贴现。

## 概率学习模型

### Preliminary预备知识

| name                        | translate                                                                                           |
|:--------------------------- |:--------------------------------------------------------------------------------------------------- |
| convex set                  | 凸集                                                                                                |
| convex optimization problem | 凸优化问题                                                                                          |
| gradient                    | $\nabla f(x)=\left[\partial f(x) / \partial x_{1} \cdots \partial f(x) / \partial x_{D}\right]^{T}$ |
| Hessian                     | $\nabla^{2} f(x)=\partial^{2} f(x) / \partial x_{i} \partial x_{j}$                                 |
| sufficient statistic        | $\theta=f(x)$                                                                                       |

### The Exponential Family指数族

指数族的概率模型：$p(x \mid \eta) =\frac{1}{Z(\eta)} \exp \left(\sum_{k=1}^{K} \eta_{k} u_{k}(x)\right) m(x) \\=\frac{1}{Z(\eta)} \exp \left(\eta^{T} u(x)\right) m(x)$，其中$Z(\eta)=\int \exp \left(\eta^{T} u(x)\right) m(x) \mathrm{d} x$是 配分函数partition function
由于非正态较易评估，广泛采用非正态分布。
指数族的分布，也被称为能量函数：$\ln \tilde{p}(x \mid \eta)=\eta^{T} u(x)+\ln m(x)$，因此指数族也可认为是对数线性函数。
对数似然函数可以写成：$\ln p(x \mid \eta)=\eta^{T} u(x)-A(\eta)+\ln m(x)$，其中$A(\eta)=lnZ(\eta)$是对数配分函数。

在后续中认为指数族是minimal representation最小表述。

指数族的一个关键性质是自然参数和平均参数之间的映射由对数配分函数的梯度给出。可以证明对数配分函数的偏导数等于相应充分统计量的均值：$\frac{\partial A(\eta)}{\partial \eta_{k}}=\mathrm{E}_{\mathrm{x} \sim p(x \mid \eta)}\left[u_{k}(\mathrm{x})\right]=\mu_{k}$，用向量形式表示为：$\nabla_{\eta} A(\eta)=\mathrm{E}_{\mathrm{x} \sim p(x \mid \eta)}[u(\mathrm{x})]=\mu$

Hessian$\nabla_{\eta}^{2} A(\eta)$与充分统计量的协方差矩阵相等，也等于自然参数的Fisher信息矩阵。

对数配分函数$A(\eta)$是严格凸函数，具有以下影响：
1.自然参数的可行值集合是凸的（平均参数的可行值的相应集合不一定）
2.自然参数$\eta$和平均参数$\mu$之间的映射是可逆的。
3.LL函数中的$lnp(x|\eta)$是$\eta$的凹函数，因此ML问题相当于凸优化问题的最大化。

Bernoulli Model伯努利模型
二元随机变量的伯努利分布为：$\operatorname{Bern}(x \mid \mu)=\mu^{x}(1-\mu)^{1-x}$
对数似然函数：$\ln (\operatorname{Bern}(x \mid \mu))=\ln \left(\frac{\mu}{1-\mu}\right) x+\ln (1-\mu)$
自然参数与平均参数之间的映射为：$\eta=\ln \left(\frac{\mu}{1-\mu}\right)$
逆映射由sigmoid函数给出:$\mu=\sigma(\eta)=\frac{1}{1+e^{-\eta}}$，S形函数通过S形将真实的转换为区间\[0，1\]，S形将小于0.5的值与参数的负值相关联，将大于0.5的值与正数相关联。
因此对数配分函数为：$A(\eta)=-\ln (1-\mu)=\ln \left(1+e^{\eta}\right)$

Categorical or Multinoulli Model多类模型
该模型的pmf分布为：$\operatorname{Cat}(x \mid \mu)=\prod_{k=1}^{C-1} \mu_{k}^{1(x=k)} \times \mu_{0}^{1-\sum_{k=1}^{C-1} 1(x=k)}$
对数似然函数可表示为：$\ln (\operatorname{Cat}(x \mid \mu))=\sum_{k=1}^{C-1} 1(x=k) \ln \frac{\mu_{k}}{\mu_{0}}+\ln \mu_{0}$
自然参数和平均参数之间的映射关系：$\eta_{k}=\ln \left(\frac{\mu_{k}}{1-\sum_{j=1}^{C-1} \mu_{j}}\right)$
再次取逆映射：$$\mu=\left[\begin{array}{c}
\frac{e^{\eta_{1}}}{1+\sum_{k=1}^{C-1} e^{\eta_{k}}} \\
\vdots \\
\frac{e^{\eta} C-1}{1+\sum_{k=1}^{C-1} e^{\eta_{k}}}
\end{array}\right]$$
充分统计的结果向量是one-hot编码：$$u(x)=\left[\begin{array}{c}
1(x=0) \\
\vdots \\
1(x=C-1)
\end{array}\right]$$
softmax函数：$$\mu=\operatorname{softmax}(\eta)=\left[\begin{array}{c}
\frac{e^{\eta_{0}}}{\sum_{k=0}^{C-1} e^{\eta_{k}}} \\
\vdots \\
\frac{e^{\eta} C-1}{\sum_{k=0}^{C-1} e^{\eta_{k}}}
\end{array}\right]$$
### Frequentist Learning频率论学习

假设N个可用数据点x的分布为：$\mathrm{x}_{n} \underset{\text { i.i.d. }}{\sim} p(x \mid \eta), n=1, \ldots, N$，用于估计平均参数和自然参数
自然参数向量使用LL函数观测：$\ln p\left(x_{\mathcal{D}} \mid \eta\right) =\sum_{n=1}^{N} \ln p\left(x_{n} \mid \eta\right) \\=-N A(\eta)+\eta^{T} \sum_{n=1}^{N} u\left(x_{n}\right)+\sum_{n=1}^{N} \ln m\left(x_{n}\right)$
忽略与$\eta$无关的量：$\ln p\left(x_{\mathcal{D}} \mid \eta\right)=-N A(\eta)+\eta^{T} u\left(x_{\mathcal{D}}\right)$

Koopman-Pitman-Darmois定理：只有指数族的充分统计数字不随着观察量$N$的增大而增大。

对数似然函数的梯度：$\frac{\partial \ln p\left(x_{\mathcal{D}} \mid \eta\right)}{\partial \eta_{k}}=u_{k}\left(x_{\mathcal{D}}\right)-N \frac{\partial A(\eta)}{\partial \eta_{k}}$
从而：$\frac{1}{N} \frac{\partial \ln p\left(x_{\mathcal{D}} \mid \eta\right)}{\partial \eta_{k}}=\frac{1}{N} u_{k}\left(x_{\mathcal{D}}\right)-\mu_{k}$
$$\begin{aligned}
\frac{1}{N} \nabla_{\eta} \ln p\left(x_{\mathcal{D}} \mid \eta\right) &=\frac{1}{N} u\left(x_{\mathcal{D}}\right)-\mu \\
&=\frac{1}{N} \sum_{n=1}^{N} u\left(x_{n}\right)-\mu
\end{aligned}$$
ML learning
ML估计值：$\mu_{M L}=\frac{1}{N} \sum_{n=1}^{N} u\left(x_{n}\right)$
根据大数定律，ML估计值会随着N的增大而更加曲线与平均参数。但当N有限时，可能会出现过拟合：当其中一个可能取值没有被观测到时，他的概率会是0.（black swan paradox or zero-count problem）

MAP learning
MAP解原则上可以通过在最优性条件$\nabla_{\eta} \ln p\left(x_{\mathcal{D}} \mid \eta\right)=0$中包括先验分布的梯度来导出。

### Bayesian Learning贝叶斯学习

使用贝叶斯定理，参数向量的后验可以写为：$p\left(\mu \mid x_{\mathcal{D}}, \alpha\right)=\frac{p(\mu \mid \alpha) p\left(x_{\mathcal{D}} \mid \mu\right)}{p\left(x_{\mathcal{D}} \mid \alpha\right)} \propto p(\mu \mid \alpha) p\left(x_{\mathcal{D}} \mid \mu\right)$

Prior distribution先验分布：
先验分布的选择：
1.Conjugate prior共轭先验，应用中更常见
2.Non-informative prior无信息先验：选取给定数据信息最少的作为先验

Beta-Bernoulli Model
可用观测分布为：$p\left(x_{\mathcal{D}} \mid \mu\right)=\prod_{n=1}^{N} \operatorname{Bern}\left(x_{n} \mid \mu\right)=\mu^{N[1]}(1-\mu)^{N[0]}$
共轭先验$β$分布为：$p(\mu \mid a, b)=\operatorname{Beta}(\mu \mid a, b) \propto \mu^{a-1}(1-\mu)^{b-1}$，其中$a$和$b$都是超参数

结果可计算如下，在后续把这些先验观察看作是“虚拟”测量，实际学习中与实际测量一起使用。
$$\begin{array}{c}
\mathrm{E}_{\mu \sim \operatorname{Beta}(\mu \mid a, b)}[\mu]=\frac{a}{a+b} \\
\operatorname{mode}_{\mu \sim \operatorname{Beta}(\mu \mid a, b)}[\mu]=\frac{a-1}{a+b-2}
\end{array}$$
计算参数向量的后验分布：$p\left(\mu \mid x_{\mathcal{D}}, a, b\right)  \propto \operatorname{Beta}(\mu \mid a+N[1], b+N[0]) \\=\mu^{N[1]+a-1}(1-\mu)^{N[0]+b-1}$
从而$\mu$的MAP估计值为：$\mu_{M A P}=\frac{a+N[1]-1}{a+b+N-2}$

Dirichlet-Categorical Model
Dirichlet-Categorical Model是将Beta-Bernoulli Model推广到离散观测的情况。
似然函数可改写为：$p\left(x_{\mathcal{D}} \mid \mu\right)=\prod_{k=0}^{C-1} \mu_{k}^{N[k]}$
共轭先验的分布是dirichlet分布：$p(\mu \mid \alpha)=\operatorname{Dir}(\mu \mid \alpha) \propto \prod_{k=0}^{C-1} \mu_{k}^{\alpha_{k}-1}$
Dirichlet分布的平均值和模式向量如下所示：$$\begin{array}{c}
\mathrm{E}_{\mu \sim \operatorname{Dir}(\mu \mid \alpha)}[\mu]=\frac{\alpha}{\sum_{j=0}^{C-1} \alpha_{j}} \\
\operatorname{mode}_{\mu \sim \operatorname{Dir}(\mu \mid \alpha)}[\mu]=\frac{\alpha-1}{\sum_{j=0}^{C-1} \alpha_{j}-C}
\end{array}$$
参数的后验值为dirichlet分布：$p\left(\mu \mid x_{\mathcal{D}}, \alpha\right) \propto \prod_{k=0}^{C-1} \mu_{k}^{N[k]+\alpha_{k}-1}=\operatorname{Dir}(\mu \mid \alpha+N)$
最终贝叶斯预测分布为：$p\left(x=k \mid x_{\mathcal{D}}, \alpha\right)=\frac{N[k]+\alpha_{k}}{N+\sum_{j=0}^{C-1} \alpha_{j}}$

Gaussian-Gaussian Model
共轭先验、似然分布均是高斯分布，后验分布也为高斯分布：$$\begin{aligned}
\mu_{N} &=\frac{\sigma^{2} / N}{\sigma_{0}^{2}+\sigma^{2} / N} \mu_{0}+\frac{\sigma_{0}^{2}}{\sigma_{0}^{2}+\sigma^{2} / N} \mu_{M L} \\
\frac{1}{\sigma_{N}^{2}} &=\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}}
\end{aligned}$$
预测分布也是高斯分布：$p\left(x \mid x_{\mathcal{D}}, \mu_{0}, \sigma_{0}^{2}\right)=\mathcal{N}\left(x \mid \mu_{N}, \sigma^{2}+\sigma_{N}^{2}\right)$

### Supervised Learning via Generalized Linear Models (GLM)基于广义线性模型的监督学习

GLM将目标变量$t$的概率定义为：$p(t \mid x, W)=\operatorname{exponential}(t \mid \eta=W x)$，其中$W$表示可学习权重的矩阵。
更一般的，可以采用x的特征向量：$p(t \mid x, W)=\operatorname{exponential}(t \mid \eta=W \phi(x))$
参数向量通过非线性向量函数将其参数化：$p(t \mid x, W)=\operatorname{exponential}(t \mid \mu=g(W \phi(x)))$

广义线性模型假设目标变量是均值$\mu = g(Wφ(x))$的“噪声”度量。
学习GLM的参数可以通过LL上的梯度上升以及求导的链式法则完成。


### Maximum Entropy Property最大熵性质

分布$p(x|\eta)$得到了所有满足约束条件的分布$p(x)$上的最大熵
数学上，分布$p(x|\eta)$可以解决优化问题：$\max _{p(x)} H(p) \text { s.t. } \mathrm{E}_{\mathrm{x} \sim p(x)}\left[u_{k}(\mathrm{x})\right]=\mu_{k} \text { for } k=1, \ldots, K$
，其中每个自然参数$\eta_k$都是与第$k$个约束相关的最优拉格朗日乘数。

### Energy-based Models基于能量的模型

将指数族的概率模型进行推广：$p(x \mid \eta)=\frac{1}{Z(\eta)} \exp \left(-\sum_{c} E_{c}\left(x_{c} \mid \eta\right)\right)$
其中$E_{c}\left(x_{c} \mid \eta\right)$被称为能量函数。每个$E_{c}\left(x_{c} \mid \eta\right)$依赖于x的其中一个子集。
当每个函数都与参数$\eta$线性相关，那么就恢复为上述指数族。
使用相关的能量值对随机变量$x_c$子集的不同配置的似然性信息进行编码：大的能量导致不可信的配置，而小的能量识别可能的配置。

对于基于能量的模型，LL梯度相对于模型参数的关键公式可改写为：
$$\frac{1}{N} \nabla_{\eta} \ln p\left(x_{\mathcal{D}} \mid \eta\right)=-\frac{1}{N} \sum_{n=1}^{N} \sum_{c} \nabla_{\eta} E_{c}\left(x_{n} \mid \eta\right)+\sum_{c} \mathrm{E}_{\mathrm{x} \sim p(x \mid \eta)}\left[\nabla_{\eta} E_{c}(\mathrm{x} \mid \eta)\right]$$
第一项为正相，点最小化的方向观察$x_D$的能量，第二项为负项，推高的能量未被注意的配置.

# 有监督学习

## 分类

### Stochastic Gradient Descent随机梯度下降
SGD优化问题形式：$\min _{\theta} \sum_{n=1}^{N} f_{n}(\theta)$
$f_n(\theta)$是代价函数，需要是可微的：$f_{n}(\theta)=\ell\left(t_{n}, \hat{t}\left(x_{n}, \theta\right)\right)$，其中$\ell$是loss function
SGD的思想是每次迭代时在代价函数的最大下降方向上移动，分为两步：
1.从全部集合中抽取小的子集
2.将最陡局部下降方向的权重更新为：$\theta^{(i)} \leftarrow \theta^{(i-1)}-\left.\frac{\gamma^{(i)}}{S} \sum_{n \in \mathcal{S}} \nabla_{\theta} f_{n}(\theta)\right|_{\theta=\theta^{(i-1)}}$

Convergence收敛
传统的梯度下降算法： 如果要优化的函数是严格凸的，对于二次损失，只要学习率不大于损失函数L的最大曲率的倒数，算法就会取得最小值。
SGD：如果选择的学习率满足Robbins-Monro条件：$\sum_{i=1}^{\infty} \gamma^{(i)}=\infty \text { and } \sum_{i=1}^{\infty}\left(\gamma^{(i)}\right)^{2}<\infty$，则SGD在优化函数严格凸的情况下一定会收敛到最优解。

1.较大的batch-size$S$减小了梯度估计的方差，因此提高了接近平稳点时的准确度。
2.当当前解远离最优解时：选择较小的batch-size可以提高收敛速度。
3.可以沿着SGD算法的迭代方向增大batch-size，作为减小步长的替代方案。

Variations and Generalizations变体和泛化
优化：
1.momentum, or heavy-ball, memory：通过考虑在最后一次更新期间获得的“动量”来校正随机梯度所建议的方向。
Nesterov momentum
2.adaptivity自适应性：根据损失函数关于每个参数的曲率的估计，对不同的参数使用不同的学习速率
AdaGrad, RMSprop and Adam
3.control variates控制变量：为了减少SGD的方差更新,加入控制变量的无偏性,不影响随机梯度与随机梯度负相关
SVRG and SAGA
4.second-order updates二阶更新：包括关于成本或目标函数的曲率的信息
Newton method、approximated Newton method，自然梯度法
5.不可微情况下：次梯度法和近端梯度、进化算法
6.从并行性和非凸性两方面优化算法


### Classification as a Supervised Learning Problem有监督问题的分类

对于一个二分类问题，有三类方法，下面依次介绍。
- 判别确定性模型：通过参数化函数$t=t(x)$直接建模域点和标签之间的确定性映射。
- 判别概率模型：通过参数化条件pmf$p(t|x)$对类$C_k$建模
- 生成概率模型：通过指定每一类的先验概率或者类条件概率密度对于联合分布进行建模

### Discriminative Deterministic Models判别确定模型

线性判别确定性分类模型具有以下形式：$\hat{t}(x, \tilde{w})=\operatorname{sign}(a(x, \tilde{w}))$
其中，激活变量（决策变量）：$a(x, \tilde{w}) =\sum_{d=1}^{D} w_{d} x_{d}+w_{0} \\=w^{T} x+w_{0}=\tilde{w}^{T} \tilde{x}$

几何解释：
判别规则确定了一个超平面（类别多的话为子空间），该超平面两侧的判别结果不同，向量$w$定义了垂直于超平面的方向，$− w 0/||w||$是决策面在方向$w$上的偏差。

给定点x，可以通过量化x和决策超平面之间的欧几里得距离进行分类（classification margin分类裕度）$|a(x, \tilde{w})| /\|w\|$
我们通过给正确分类的点一个正号和给不正确分类的点一个负号来扩充裕度的定义。假设$t$取$\{−1，1\}$中的值，则几何裕度的定义为：$\frac{t \cdot a(x, \tilde{w})}{\|w\|}$，其绝对值等于分类余量。
边际函数定义：$t \cdot a(x, \tilde{w})$

缺点：
- 误差：有的协变量在空间中不是线性可分的，选择此类模型会产生较大的平均损失
- 过拟合：当$D$较大且数据点$N$不足时，学习分类器的$D+1$个权重可能导致过拟合
- 域点数目与数据有关：数据点的维度可能随着数据的变化而变化

 基于特征的模型：$a(x, \tilde{w})=\sum_{k=1}^{D^{\prime}} w_{k} \phi_{k}(x)=\tilde{w}^{T} \phi(x)$

Learning学习

通过在权重向量$w$上引入正则化函数$R(w)$来控制过拟合，通过求解正则化的ERM问题$\min _{\tilde{w}} L_{\mathcal{D}}(\tilde{w})+\frac{\lambda}{N} R(\tilde{w})$
学习确定性因子$\tilde{w}$，其中经验风险为：$L_{\mathcal{D}}(\tilde{w})=\frac{1}{N} \sum_{n=1}^{N} \ell\left(t_{n}, \hat{t}\left(x_{n}, \tilde{w}\right)\right)$
1.正则化项不一定是可微的，如一范数
2.损失函数选择0-1损失，则泛化损失是分类错误的概率

使用可微的替代激活函数改写ERM问题：$\min _{\tilde{w}} \sum_{n=1}^{N} \ell\left(t_{n}, a\left(x_{n}, \tilde{w}\right)\right)+\frac{\lambda}{N} R(\tilde{w})$
理想情况下，替代损失函数也应该是原始损失函数的上界。

Perceptron Algorithm感知器算法
感知器算法定义了替代感知器损失函数：$\ell(t, a(x, \tilde{w}))=\max (0,-t \cdot a(x, \tilde{w}))$
感知器损失将零成本分配给正确分类的样本$x$，其功能边际$t · a(x,w)$为正，而将等于功能边际的绝对值的成本分配给错误分类的样本，其功能边际为负。

算法流程如下：
1.初始化权重$\tilde{w}^{(0)}$
2.从训练集中任意挑选一个训练样本$(x_n,t_n)$进行替换
3.如果分类正确，即$t_{n} a\left(x_{n}, \tilde{w}\right) \geq 0$，不更新权重$\tilde{w}^{(i)} \leftarrow \tilde{w}^{(i-1)}$
4.如果分类错误，即$t_{n} a\left(x_{n}, \tilde{w}\right) < 0$，更新权重：$\tilde{w}^{(i)} \leftarrow \tilde{w}^{(i-1)}-\left.\nabla_{\tilde{w}} \ell\left(t_{n}, a\left(x_{n}, \tilde{w}\right)\right)\right|_{\tilde{w}=\tilde{w}^{(i-1)}}=\tilde{w}^{(i-1)}+\phi\left(x_{n}\right) t_{n}$

问题：
1.对于线性可分的训练集可能收敛速度较慢
2.在非线性可分的训练集上不能使用


Support Vector Machine (SVM)支持向量机
激活函数可以改写为：$a(x, \tilde{w})=w_{0}+w^{T} \phi(x)$，$w_0$是偏置权重
采用hinge loss function铰链损失替代ERM问题中的损失函数：$\ell(t, a(x, \tilde{w}))=\max (0,1-t \cdot a(x, \tilde{w}))$
正则化函数：$R(\tilde{w})=\|w\|^{2}$，只涉及向量$w$，不涉及偏置权重。

与感知器算法区别：
1.SVM包括正则化项，在泛化误差方面有理论支撑。
2.SVM尝试使用强大的凸优化技术直接解决正则化ERM问题，而不是依赖于SGD

假设$z_n \geq \ell\left(t_{n}, a\left(x_{n}, \tilde{w}\right)\right)$，优化问题可改写为：
$$\begin{array}{l}
\min _{\tilde{w}, z} \sum_{n=1}^{N} z_{n}+\frac{\lambda}{N}\|w\|^{2} \\
\text { s.t. } t_{n} \cdot a\left(x_{n}, \tilde{w}\right) \geq 1-z_{n} \\
\quad z_{n} \geq 0 \text { for } n=1, \ldots, N
\end{array}
$$
从而可以得到最优解：$z_{n}=\ell\left(t_{n}, a\left(x_{n}, \tilde{w}\right)\right)$

对于Linearly separable sets and support vectors线性可分训练集与支持向量机，优化问题可以进一步改写：
$$\begin{array}{l}
\min _{\tilde{w}}\|w\|^{2} \\
\text { s.t. } t_{n} \cdot a\left(x_{n}, \tilde{w}\right) \geq 1 \text { for } n=1, \ldots, N
\end{array}$$
上式可以解释为在所有训练点上maximization of the minimum geometric margin最小几何边缘的最大化，其计算公式为：$\min _{n=1, \ldots, N} \frac{t_{n} a\left(x_{n}, \tilde{w}\right)}{\|w\|}=\frac{1}{\|w\|}$

Kernel Methods核技巧
可以将SVM的优化问题通过对偶变量或拉格朗日乘数进行优化，所得到的最佳激活可以表示为：$a(x, \tilde{w})=\sum_{n=1}^{N} \alpha_{n} t_{n} k\left(x, x_{n}\right)$，其中$\alpha_n$是最优对偶变量，核函数：$k(x, y)=\phi(x)^{T} \phi(y)$

核函数是测量两个数据点的相关性的任何对称函数，可能在无限维空间中。

多项式核：$k(x, y)=\left(\gamma x^{T} y+r\right)^{L}$
高斯核：$k(x, y)=e^{-r\|\mathrm{x}-y\|^{2}}$

Multi-Class Classification多分类问题

可以通过构建多个二分类模型，其定义主要分为两种：one-versus-the-rest、one-versus-one

### Discriminative Probabilistic Models: Generalized Linear Models判别概率模型：广义线性模型

区别性概率模型潜在地比确定性模型更强大，因为它们允许对输入变量的标签分配中的不确定性源进行建模。这种随机性可以模拟噪声、标记错误，和/或由于有限数据的可用性而导致的分类规则中的剩余不确定性。概率模型还可以通过在可能的标签值上产生概率分布来更自然地适应多于两个类别的存在。

Model
对于分类，标签t可以取有限个值，因此在二元情况下可以用伯努利变量来描述，或者更一般地说，可以用范畴变量来描述。
二元分类的GLM称为logistic回归，假设预测分布为：$p(t=1 \mid x)=\sigma\left(\tilde{w}^{T} \phi(x)\right)$，则$p(t=0 \mid x)=1-\sigma\left(\tilde{w}^{T} \phi(x)\right)=\sigma\left(-\tilde{w}^{T} \phi(x)\right)$
自然参数向量：$\eta=\tilde{w}^{T} \phi(x)$
判别规则：$p\left(\mathcal{C}_{1} \mid x\right)=p(t=1 \mid x) \underset{\mathcal{C}_{0}}{\stackrel{\mathcal{C}_{1}}{\gtrless}} \frac{1}{2}$或者$\tilde{w}^{T} \phi(x) \underset{\mathcal{C}_{0}}{\stackrel{\mathcal{C}_{1}}{\gtrless}} 0$
Learning
ML
NLL函数可写为：$$\begin{aligned}
-\ln p\left(t_{\mathcal{D}} \mid x_{\mathcal{D}}, \tilde{w}\right) & =-\sum_{n=1}^{N} \ln p\left(t_{n} \mid x_{n}, \tilde{w}\right) \\
& =-\sum_{n=1}^{N}\left\{t_{n} \ln \left(y_{n}\right)+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\}
\end{aligned}$$










## 统计学习理论

# 无监督学习


## 无监督学习

# 高级建模和推理

## 概率图模型

## 近似推理与学习



